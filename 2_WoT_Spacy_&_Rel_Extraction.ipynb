{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty list for graphs from books\n",
    "books_graph = []\n",
    "all_books = [b for b in os.scandir('/directory') if '.txt' in b.name]\n",
    "# Sort dir entries by number\n",
    "all_books.sort(key=lambda x: int(x.name.split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(file_name):\n",
    "    \"\"\"\n",
    "    Function to process text from a text file (.txt) using Spacy.\n",
    "\n",
    "    Params:\n",
    "    file_name -- name of a txt file as string\n",
    "\n",
    "    Returns:\n",
    "    a processed doc file using Spacy English language model\n",
    "\n",
    "    \"\"\"\n",
    "    # Load spacy English languague model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 2185000\n",
    "    book_text = open(file_name).read()\n",
    "    book_doc = nlp(book_text)\n",
    "    return book_doc\n",
    "\n",
    "\n",
    "def get_ne_list_per_sentence(spacy_doc):\n",
    "    \"\"\"\n",
    "    Get a list of entites per sentence of a Spacy document and store in a dataframe.\n",
    "\n",
    "    Params:\n",
    "    spacy_doc -- a Spacy processed document\n",
    "\n",
    "    Returns:\n",
    "    a dataframe containing the sentences and corresponding list of recognised named entities       in the sentences\n",
    "    \"\"\"\n",
    "\n",
    "    sent_entity_df = []\n",
    "\n",
    "    # Loop through sentences, store named entity list for each sentence\n",
    "    for sent in spacy_doc.sents:\n",
    "        entity_list = [ent.text for ent in sent.ents]\n",
    "        sent_entity_df.append({\"sentence\": sent, \"entities\": entity_list})\n",
    "\n",
    "    sent_entity_df = pd.DataFrame(sent_entity_df)\n",
    "\n",
    "    return sent_entity_df\n",
    "\n",
    "\n",
    "def filter_entity(ent_list, character_df, locations):\n",
    "    \"\"\"\n",
    "    Function to filter out non-character entities.\n",
    "\n",
    "    Params:\n",
    "    ent_list -- list of entities to be filtered\n",
    "    character_df -- a dataframe contain characters' names and characters' first names\n",
    "    locations -- list of locations\n",
    "\n",
    "    Returns:\n",
    "    a list of entities that are characters (matching by names or first names).\n",
    "\n",
    "    \"\"\"\n",
    "    return [ent for ent in ent_list\n",
    "            if ent in list(character_df.character)\n",
    "            or ent in list(character_df.character_firstname)\n",
    "            and ent not in locations]\n",
    "\n",
    "\n",
    "def create_relationships(df, window_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a dataframe of relationships based on the df dataframe (containing lists of chracters per sentence) and the window size of n sentences.\n",
    "\n",
    "    Params:\n",
    "    df -- a dataframe containing a column called character_entities with the list of chracters for each sentence of a document.\n",
    "    window_size -- size of the windows (number of sentences) for creating relationships between two adjacent characters in the text.\n",
    "\n",
    "    Returns:\n",
    "    a relationship dataframe containing 3 columns: source, target, value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    relationships = []\n",
    "\n",
    "    for i in range(df.index[-1]):\n",
    "        end_i = min(i + window_size, df.index[-1])\n",
    "        char_list = sum((df.loc[i: end_i].character_entities), [])\n",
    "\n",
    "        # Remove duplicated characters that are next to each other\n",
    "        char_unique = [char_list[i] for i in range(len(char_list))\n",
    "                       if (i==0) or char_list[i] != char_list[i-1]]\n",
    "\n",
    "        if len(char_unique) > 1:\n",
    "            for idx, a in enumerate(char_unique[:-1]):\n",
    "                b = char_unique[idx + 1]\n",
    "                relationships.append({\"source\": a, \"target\": b})\n",
    "\n",
    "    relationship_df = pd.DataFrame(relationships)\n",
    "    # Sort the cases with a->b and b->a\n",
    "    relationship_df = pd.DataFrame(np.sort(relationship_df.values, axis = 1),\n",
    "                                   columns = relationship_df.columns)\n",
    "    relationship_df[\"value\"] = 1\n",
    "    relationship_df = relationship_df.groupby([\"source\",\"target\"],\n",
    "                                              sort=False,\n",
    "                                              as_index=False).sum()\n",
    "\n",
    "    return relationship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load character names from characters_wot.csv\n",
    "character_df = pd.read_csv(\"/directory/characters_wot.csv\")\n",
    "character_df['character_firstname'] = character_df['character'].apply(lambda x: x.split(' ', 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of locations extracted from the scraping of locations and nations\n",
    "locations = ['Aiel Waste','Almoth Plain','Arindrim','Asnelle','Black Hills','Blasted Lands','Caralain Grass','Comaidin Riots','Drowned Lands',\n",
    "             'Ganai','Great Blight','Haddon Mirk','Hills of Absher','Hills of Kintara','Kaensada Hills',\"Kinslayer's Dagger\",'Kiranaille',\n",
    "             'Kunwar','Larcheen','Mad Lands','Maram Kashor','Plain of Lances','Plains of Maredo','Pujili',\"Sa'las Plains\",'Shadow Coast',\n",
    "             'Shahayni','Sharaman',\"Strangers' Markets\",'Termool','Toman Head','Two Rivers',\"World's End\",'Almoth Plain','Altara','Amadicia',\n",
    "             'Amayar','Andor','Arad Doman','Arafel',\"Atha'an Miere\",'Borderlands','Cairhien', 'Ghealdan', 'Illian','Kandor','Mad Lands', 'Malkier',\n",
    "             'Murandy','Saldaea','Seanchan','Shara', 'Shienar','Tarabon','Tear','Westlands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through book list and create graphs\n",
    "for book in tqdm(all_books, total=len(all_books)):\n",
    "    book_text = ner(book)\n",
    "\n",
    "    # Get list of entities per sentences\n",
    "    sent_entity_df = get_ne_list_per_sentence(book_text)\n",
    "\n",
    "    # Select only character entities\n",
    "    sent_entity_df['character_entities'] = sent_entity_df['entities'].apply(lambda x: filter_entity(x, character_df, locations))\n",
    "\n",
    "    # Filter out sentences that don't have any character entities\n",
    "    sent_entity_df_filtered = sent_entity_df[sent_entity_df['character_entities'].map(len) > 0]\n",
    "\n",
    "    # Take only first name of characters\n",
    "    sent_entity_df_filtered['character_entities'] = sent_entity_df_filtered['character_entities'].apply(lambda x: [item.split()[0]\n",
    "                                                                                                               for item in x])\n",
    "\n",
    "    # Create relationship df\n",
    "    relationship_df = create_relationships(df = sent_entity_df_filtered, window_size = 5)\n",
    "\n",
    "    # Create a graph from a pandas dataframe\n",
    "    G = nx.from_pandas_edgelist(relationship_df,\n",
    "                                source = \"source\",\n",
    "                                target = \"target\",\n",
    "                                edge_attr = \"value\",\n",
    "                                create_using = nx.Graph())\n",
    "\n",
    "    books_graph.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes of relations for each book\n",
    "dataframes = []\n",
    "\n",
    "for graph in books_graph:\n",
    "    dataframe = nx.to_pandas_edgelist(graph)\n",
    "    dataframes.append(dataframe)\n",
    "\n",
    "\n",
    "for i, dataframe in enumerate(dataframes):\n",
    "    dataframe.to_csv(f\"dataframe_{i}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
